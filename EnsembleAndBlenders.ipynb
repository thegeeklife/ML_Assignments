{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "623MachineLearningHomework5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWCacTarilIL7fuD/g6xOx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thegeeklife/ML_Assignments/blob/main/EnsembleAndBlenders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXKwpa30k6pp"
      },
      "source": [
        "### CNIT 623 003 Machine Learning \n",
        "#### Homework 5\n",
        "#### Subia Ansari"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKMLU2ED41m5"
      },
      "source": [
        "import sys\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "np.random.seed(623)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfUO6OMSsgmP"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsXIk33is4x4",
        "outputId": "50698e97-cd9a-49cf-9852-4f9dbb52aee0"
      },
      "source": [
        "X, y = mnist['data'], mnist['target']\n",
        "X.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JueKogJ3tHHF"
      },
      "source": [
        "# convert y from string to integer\n",
        "y = y.astype(np.uint8)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wo5MQ1__dju"
      },
      "source": [
        "### Splitting into train, test, validaiton, holdout sets.\n",
        "\n",
        "`[stacking]:  Use the hold-out set H (20,000) from HW4 as the new training set for a blender function of those three classifiers trained in HW4. Choose two different classifier as your choice of the blender,  which takes the input vector from the predictions of layer 1 classifiers on the new 20,000 training set. Evaluate the results of stacking on the validation set V (10,000) and choose the better blender of the two as the final stacking implementation. Evaluate the results of each individual classifier and the stacking classifier over the test set T(10,000).  How does the stacking implementation compare to the soft voting classifier in HW4?`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNBYvuYHtPqM"
      },
      "source": [
        "#test set 10k instances\n",
        "X_h_v , X_test, y_h_v, y_test = train_test_split(X,y,test_size=10000, \n",
        "                                           random_state = 623)\n",
        "\n",
        "#validation : 10K\n",
        "X_train_h, X_val, y_train_h, y_val = train_test_split(X_h_v, y_h_v, test_size = 10000, random_state=623)\n",
        "\n",
        "#Training and holdout sets : 30k train; 20k holdout\n",
        "X_train, X_hold, y_train, y_hold = train_test_split(X_train_h, y_train_h, test_size = 20000, random_state=623)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39CeW1q4DA2A"
      },
      "source": [
        " In homework 4, I created an ensemble comprising of KNN Classifier, Decision Tree Classifier  and Logisitc Regression after removing the weakest learner, Naive Bayes Classifier. The ensemble gave an accuracy of 93.9% (hard voting)\n",
        "\n",
        "We will use the same three models now for our L1 Classifier -\n",
        "\n",
        "1. Logistic Regression\n",
        "2. Decision Tree\n",
        "3. KNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BIjPjmNC6n_"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "# scale the data so that we don't get convergence warning\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "#Scaling the data before training\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_hold_scaled = scaler.transform(X_hold)\n",
        "\n",
        "#Declaring classifiers\n",
        "log_clf = LogisticRegression(solver='lbfgs', random_state=623)\n",
        "dtree_clf = DecisionTreeClassifier(random_state=623) #using default gini and max depth\n",
        "knn_clf = KNeighborsClassifier(weights='distance', n_neighbors=4)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwFHpfxnEoPF",
        "outputId": "ec6fb0a5-c061-4d19-95af-5ec6c4e7ea8f"
      },
      "source": [
        "classifiers = [log_clf, dtree_clf, knn_clf]\n",
        "\n",
        "for clf in classifiers:\n",
        "  print(\"Training the \", clf)\n",
        "  clf.fit(X_train_scaled, y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training the  LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=623, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training the  DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
            "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=2,\n",
            "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
            "                       random_state=623, splitter='best')\n",
            "Training the  KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=4, p=2,\n",
            "                     weights='distance')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaxOdwXXFVuu"
      },
      "source": [
        "#Scores of the models\n",
        "\n",
        "scores = [clf.score(X_val_scaled, y_val) for clf in classifiers]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XXjF998Kc0a",
        "outputId": "a958d29b-2998-4328-919a-4273c72358e9"
      },
      "source": [
        "scores"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9051, 0.8517, 0.9421]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_iyuM7MJN1-"
      },
      "source": [
        "Layer 1 predictions on hold out set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2RIaz_FJFOK"
      },
      "source": [
        "X_hold_pred = np.empty((len(X_hold_scaled), len(classifiers)), dtype=np.float32)\n",
        "\n",
        "\n",
        "for index, clf in enumerate(classifiers):\n",
        "  X_hold_pred[:, index] = clf.predict(X_hold_scaled)\n",
        "\n",
        "hold_scores = [clf.score(X_hold_scaled, y_hold)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTURsXnrLye5"
      },
      "source": [
        "### Use Blender\n",
        "\n",
        "I am choosing Random Forest and XGBoost as my blenders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKr7QjlHL-6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea6de23f-bd4c-4fd9-a9ba-669839b1c2e2"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "\n",
        "rf_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=623)\n",
        "gb_blender = GradientBoostingClassifier(n_estimators=200, random_state=623)\n",
        "\n",
        "blenders = [rf_blender, gb_blender]\n",
        "\n",
        "rf_blender.fit(X_hold_pred, y_hold)\n",
        "gb_blender.fit(X_hold_pred, y_hold)\n",
        "\n",
        "b_rf_score = rf_blender.oob_score_ #oob score\n",
        "gen_rf_score = rf_blender.score(X_hold_pred, y_hold) #score to compare with Gradient Boosting Classifier\n",
        "\n",
        "print('Random forest scores - oob : ', b_rf_score, ' Score : ', gen_rf_score)\n",
        "#for b in blenders:\n",
        " # b.fit(X_hold_pred, y_hold)\n",
        "  #b_scores = [b.oob_score_ for b in blenders]\n",
        "\n",
        "b_gb_score = gb_blender.score(X_hold_pred, y_hold)\n",
        "print('Gradient Boosting Classifier Score : ', b_gb_score)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random forest scores - oob :  0.94465  Score :  0.9585\n",
            "Gradient Boosting Classifier Score :  0.95425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lI4Qv6byCV7k"
      },
      "source": [
        "Generating prediction results over 10K test data from the three classifiers in layer 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPt66fIP92ST",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95cfa5e2-cbfc-4bac-e209-666c78fa86c6"
      },
      "source": [
        "X_test_predictions = np.empty((len(X_test), len(classifiers)), dtype=np.float32)\n",
        "\n",
        "for index, clf in enumerate(classifiers):\n",
        "  X_test_predictions[:, index] = clf.predict(X_test)\n",
        "\n",
        "y_pred_rf = rf_blender.predict(X_test_predictions)\n",
        "y_pred_gb = gb_blender.predict(X_test_predictions)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "rf_acc = accuracy_score(y_test, y_pred_rf)\n",
        "gb_acc = accuracy_score(y_test, y_pred_gb)\n",
        "\n",
        "print('1. Rf Accuracy = ', rf_acc, ' | 2. Gb Accuracy = ', gb_acc)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Rf Accuracy =  0.7906  | 2. Gb Accuracy =  0.8023\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF5T9HHyEuoO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}